<!doctype html>
<html>
  <head>
    <meta charset="utf-8">

    <title>Natasha — качественный компактный NER для русского языка</title>
    <meta name="title" content="Natasha — качественный компактный NER для русского языка">
    <meta property="og:title" content="Natasha — качественный компактный NER для русского языка">
    <meta property="twitter:title" content="Natasha — качественный компактный NER для русского языка">

    <meta name="description" content="Извлечение имён, названий топонимов и организаций из новостных статей">
    <meta property="og:description" content="Извлечение имён, названий топонимов и организаций из новостных статей">
    <meta property="twitter:description" content="Извлечение имён, названий топонимов и организаций из новостных статей">

    <meta name="keywords" content="ner, извлечение именованных сущностей, русский язык, новости, имена, топонимы, организации">

    <meta property="og:type" content="website">
    <meta property="twitter:card" content="summary_large_image">

    <meta property="og:url" content="https://natasha.github.io/ner/">
    <meta property="twitter:url" content="https://natasha.github.io/ner/">

    <meta property="og:image" content="https://natasha.github.io/ner/images/preview.png">
    <meta property="twitter:image" content="https://natasha.github.io/ner/images/preview.png">

    <link rel="icon" href="/images/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href="/styles/bootstrap.min.css">
    <link rel="stylesheet" href="style.css">

    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-P65FXVJ');</script>
    <!-- End Google Tag Manager -->
  </head>
  <body>
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P65FXVJ"
		      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

    <div class="container">
      <div class="row">
      	<div class="col-8">
      	  <p>
      	    <a href="/">
      	      <span class="hanging-arrow">←</span> Проект Natasha
      	    </a>
      	  </p>

      	  <h1>Natasha — качественное компактное решение для извлечения именованных сущностей из новостных статей на русском языке</h1>
      	</div>
      </div>

      <div class="row">
      	<div class="col-6">
      	  <p>
      	    <a href="https://github.com/natasha/natasha">Библиотека Natasha</a> решает базовые задачи обработки естественного русского языка: сегментация на токены и предложения, морфологический и синтаксический анализ, лемматизация, извлечение именованных сущностей. Для новостных статей качество на всех задачах <a href="https://github.com/natasha/natasha#evaluation">сравнимо или превосходит существующие решения</a>. Библиотека поддерживает Python 3.5+ и PyPy3, не требует GPU, зависит только от NumPy.
      	  </p>
      	</div>
      </div>

      <div class="row">
      	<div class="col-6">
      	  <p>
      	    В этой статье разберёмся, как Natasha решает задачу извлечения именованных сущностей. Стенд демонстрирует поиск подстрок с <span class="box PER">именами</span>, названиями <span class="box LOC">топонимов</span> и <span class="box ORG">организаций</span>:
      	  </p>
      	</div>
      	<div class="col-6 note">
      	  Модель обучена на текстах новостей. На других тематиках качество хуже. Демо-стенд отвечает с задержкой, обрабатывает первые 1000 слов.
      	</div>
      </div>
      
      <div id="demo">
      	<div class="row">
      	  <div class="scroll col-6">
      	    <div id="editor">
      	      <div contenteditable="true" id="text">Бурятия и Забайкальский край переданы из Сибирского федерального округа (СФО) в состав Дальневосточного (ДФО). Соответствующий указ подписал президент Владимир Путин, документ опубликован на официальном интернет-портале правовой информации. Этим же указом глава государства поручил руководителю своей администрации утвердить структуру и штатную численность аппаратов полномочных представителей президента в этих двух округах. После исключения Бурятии и Забайкалья в составе СФО остались десять регионов: Алтай, Алтайский край, Иркутская, Кемеровская, Новосибирская, Омская и Томская области, Красноярский край, Тува и Хакасия. Действующим полпредом президента в этом округе является бывший губернатор Севастополя, экс-заместитель командующего Черноморским флотом России Сергей Меняйло. В составе ДФО отныне 11 субъектов. Помимо Бурятии и Забайкалья, это Камчатский, Приморский и Хабаровский края, Амурская, Еврейская автономная, Магаданская и Сахалинская области, а также Якутия и Чукотка. Дальневосточное полпредство возглавляет Юрий Трутнев, совмещающий эту должность с постом вице-премьера в правительстве России. Федеральные округа были созданы в мае 2000 года в соответствии с указом президента Путина.</div>
      	      <div id="markup"></div>
      	    </div>
      	  </div>
      	  <div class="scroll col-6">
      	    <div id="facts"></div>
      	  </div>
      	</div>

      	<div id="controls" class="row">
      	  <div id="examples" class="col-6">
      	    Примеры:
      	    <a href="#" data-text="Бурятия и Забайкальский край переданы из Сибирского федерального округа (СФО) в состав Дальневосточного (ДФО). Соответствующий указ подписал президент Владимир Путин, документ опубликован на официальном интернет-портале правовой информации. Этим же указом глава государства поручил руководителю своей администрации утвердить структуру и штатную численность аппаратов полномочных представителей президента в этих двух округах. После исключения Бурятии и Забайкалья в составе СФО остались десять регионов: Алтай, Алтайский край, Иркутская, Кемеровская, Новосибирская, Омская и Томская области, Красноярский край, Тува и Хакасия. Действующим полпредом президента в этом округе является бывший губернатор Севастополя, экс-заместитель командующего Черноморским флотом России Сергей Меняйло. В составе ДФО отныне 11 субъектов. Помимо Бурятии и Забайкалья, это Камчатский, Приморский и Хабаровский края, Амурская, Еврейская автономная, Магаданская и Сахалинская области, а также Якутия и Чукотка. Дальневосточное полпредство возглавляет Юрий Трутнев, совмещающий эту должность с постом вице-премьера в правительстве России. Федеральные округа были созданы в мае 2000 года в соответствии с указом президента Путина.">Путин уменьшил Сибирский федеральный округ</a>,

      	    <a href="#" data-text="На спутниковых снимках Google Earth, сделанных в конце ноября и демонстрирующих пригород Каменск-Шахтинского в Ростовской области, заметны «сотни российских танков у границы с Украиной», пишет Defence Blog.  Издание отмечает, что военная техника расположена всего в 18 километрах от российско-украинской границы. Кроме сотен танков, прежде всего Т-64 и Т-62М, на снимках можно рассмотреть, в частности, несколько тысяч военных грузовиков. В декабре президент Украины Петр Порошенко заявил, что десантно-штурмовые подразделения Вооруженных сил Украины будут переброшены на границу с Россией. В конце ноября Порошенко своим указом ввел военное положение в ряде регионов Украины, граничащих с Россией и Приднестровьем — Винницкой, Луганской, Николаевской, Одесской, Сумской, Харьковской, Черниговской, Херсонской, Донецкой и Запорожской областях, а также во внутренних водах Азовско-Керченской акватории.">«Танковая орда» на границе России с Украиной попала на спутниковые снимки</a>,

      	    <a href="#" data-text="Башкан (глава) Гагаузской автономии в составе Молдавии Ирина Влах отказалась пожимать протянутую руку президента Молдавии Игоря Додона. На опубликованное в Facebook видео обратило внимание Deschide. Это произошло 18 октября, во время визита Додона и президента Турции Реджепа Тайипа Эрдогана в столицу Гагаузии Комрат. На мероприятиях по случаю визита Влах прошла мимо сидящего с супругой Додона, проигнорировав протянутую ей руку. Осознав, что башкан Гагаузии не собирается его приветствовать, молдавский лидер перевел взгляд на часы. В тот же день Додон и Эрдоган приняли участие в пресс-конференции в Кишиневе. В ходе брифинга турецкий президент задремал.">Додона оставили с протянутой рукой</a>,

      	    <a href="#" data-text="Специалисты признали македонский Тетово самым грязным европейским городом. Данные приведены на сайте Numbeo в рейтинге Pollution Index 2018 Mid-Year. Список составлен на основе анализа экологии 76 городов Европы. Показатель загрязнения Тетово — 97,57 пункта. Вторую позицию занял итальянский Неаполь (84,61 пункта). На третьем месте — столица Македонии Скопье с индексом 82,17 пункта. Также в топ-10 вошли албанская столица Тирана, итальянский Турин, румынский Бухарест, столица Боснии и Герцеговины Сараево, польский Краков, Пловдив (Болгария), а также столица Украины Киев. Санкт-Петербург занял 17-е место, обогнав Москву по уровню загрязнения на две позиции. Самыми благоприятными для жизни с точки зрения экологии стали две европейские столицы — Хельсинки (Финляндия) и Рейкьявик (Исландия). В марте столица Ирака Багдад была признана худшим в мире городом для проживания по версии международной консалтинговой компании Mercer. Следом за Багдадом в списке из 450 населенных пунктов идет Банги — столица Центральноафриканской Республики (ЦАР).">Назван самый грязный город Европы</a>,

      	    <a href="#" data-text="В Москве на вечеринке «Крыши мира» в «Бессонице» за диджейский пульт встанет канадский музыкант Art Department. Об этом «Ленте.ру» сообщили организаторы. Мероприятие пройдет в пятницу, 16 ноября. Art Department — проект канадского музыканта Джонни Уайта. В прошлом году Уайт выступил на вечеринках Circoloco и Elrow, отыграл в берлинском Watergate и Hï на Ибице, а также был заявлен в качестве одного из хедлайнеров фестиваля BPM в Португалии. Art Department — постоянный участник вечеринок Paradise, знаковых шоукейсов Джейми Джонса. Помимо диджеинга, Уайт ведет свой лейбл No.19 Music, объединивший таких музыкантов, как Мэтью Джонсон, Джейми Джонс, Martinez Brothers и Дэннис Феррер. Заказать билеты можно по ссылке.">На вечеринке в Москве выступит Art Department</a>,

      	    <a href="#" data-text="Президент России Владимир Путин предложил Георгию Полтавченко покинуть пост губернатора Санкт-Петербурга. Об этом в среду, 3 октября, сообщил пресс-секретарь главы государства Дмитрий Песков, его слова приводит ТАСС. По словам Пескова, занять место врио губернатора предложено полномочному представителю президента в Северо-Западном федеральном округе Александру Беглову. «Владимир Путин только что провел встречу с Бегловым и Полтавченко. Путин предложил Полтавченко возглавить ОСК (Объединенную судостроительную корпорацию — прим. «Ленты.ру») в качестве председателя совета директоров компании, а Беглову предложил стать исполняющим обязанности губернатора Санкт-Петербурга до сентября, до выборов», — пояснил представитель Кремля. Как уточнили в пресс-службе Кремля, президент уже подписал соответствующий указ. Полтавченко возглавлял администрацию Петербурга с 2011 года. Беглов был назначен полномочным представителем президента в СЗФО в 2017 году. До этого в течение пяти лет он занимал аналогичную должность в Центральном федеральном округе. Накануне, 2 октября, о сложении полномочий сообщил губернатор Липецкой области Олег Королев, а также глава Курганской области Алексей Кокорин. Исполняющими обязанности глав регионов были назначены Игорь Артамонов и Вадим Шумков соответственно.">Путин убрал Полтавченко с поста губернатора Петербурга</a>,

      	    <a href="#" data-text="Председатель Банка России Эльвира Набиуллина посетила Университет спецназа в Чечне. Видеоотчет о ее визите опубликован в Instagram учебного заведения. Глава Центробанка понаблюдала за занятиями по огневой и тактико-огневой подготовке, побывала на уроках практической стрельбы. Кроме того, Набиуллину прокатили на багги «Чаборз М-3». Руководитель ЦБ приехала в Чечню в субботу, 29 сентября, для участия в заседании межбанковского совета Центробанков России и Белоруссии.">Набиуллину в Чечне покатали на багги</a>,

      	    <a href="#" data-text="Абсолютный бойцовский чемпионат (UFC) объявил, что американец Майкл Джонсон станет соперником россиянина Артема Лобова в бою на турнире UFC Fight Night. Изначально с Лобовым должен был драться его соотечественник Зубайра Тухугов. Поединок состоится 27 октября. «Майкл Джонсон заменит Зубайру Тухугова в бою с Артемом Лобовым из-за расследования, которое проводит Атлетическая комиссия штата Невада», — говорится в сообщении. Тухугов — секундант россиянина Хабиба Нурмагомедова. Глава промоушена Дэйна Уайт после победы Нурмагомедова над Конором Макгрегором пообещал, что бойцы, напавшие на ирландца после боя, будут изгнаны из UFC. Тухугов был одним из тех, кто атаковал Макгрегора. Из-за этих слов между UFC и Нурмагомедовым возник конфликт: россиянин пригрозил разорвать свой контракт, если промоушен откажется от Тухугова. 17 октября Дэйна Уайт сообщил, что Нурмагомедов останется в организации.">UFC официально отстранил напавшего на Макгрегора российского бойца</a>,

      	    <a href="#" data-text="Испания не будет блокировать сделку по выходу Великобритании из Европейского союза из-за вопроса Гибралтара. Об этом сообщил премьер-министр Испании Педро Санчес, передает AFP. «Я только что сообщил королю, что Испания достигла соглашения по Гибралтару. Завтра состоится заседание Европейского Совета. Европа и Соединенное Королевство приняли требования Испании. Испания отменила вето и будет голосовать за Brexit», — сказал Санчес в субботу, 24 ноября. Глава Европейского совета Дональд Туск сказал, что сделка снизит «риски и потери» от выхода Британии из Евросоюза. «Хотя в этот день ни у кого не будет причин для счастья, я хотел бы подчеркнуть одну вещь: в этот критический момент 27 членов ЕС прошли тест на единство и солидарность», — сказал Туск. Ранее Санчес заявлял, что Мадрид может блокировать соглашение о выходе Британии из ЕС, если в нем не будет прописан статус Гибралтара — британской территории на Пиренейском полуострове.">Испания согласилась поддержать Brexit</a>,

      	    <a href="#" data-text="Движение «Талибан» (запрещено в России) выбрало представителей, которые отправятся в Москву на консультации по ситуации в Афганистане. Об этом сообщает телеканал 1TV на своей странице в Twitter. По его данным, в российскую столицу должны прибыть пять человек: Мухаммад Аббас Станикзай, Салам Ханафи, Шахабуддин Делавар, Зия-ур-Рахман Мадани и Мухаммад Сохаил Шахин. Отмечается, что они не собираются проводить никаких официальных переговоров с делегацией из Кабула. Ранее в МИД России сообщили, что заседание состоится 9 ноября в рамках Московского формата консультаций по Афганистану. Участие в нем примут заместители министров иностранных дел и спецпредставители ряда государств. Приглашения были направлены Афганистану, Индии, Ирану, Казахстану, Киргизии, Китаю, Пакистану, Таджикистану, Туркменистану, Узбекистану, а также США. «Талибан» образовался в 1994 году в разгар Афганской войны. В 1996-2001 годах талибы находились у власти в стране, а после свержения в 2001-м начали вести партизанскую войну с правительственными войсками и силами НАТО в Афганистане и Пакистане. В настоящее время движение и афганское правительство ищут способы достичь политического компромисса, однако столкновения между ними продолжаются. В 2003 году Совет Безопасности ООН и Верховный суд России признали «Талибан» террористической организацией.">В Москву пожалуют пять талибов</a>.
      	  </div>

      	  <div class="column col-6">
      	    <div>
      	      <button class="btn btn-light" type="button">
      		<div id="running">
      		  <span class="spinner-border spinner-border-sm"></span>
      		  Обработка
      		</div>
      		<div id="run">
      		  Обработать
      		</div>
      	      </button>
      	      <span>(Shift+Enter)</span>
      	    </div>
      	  </div>
      	</div>
      </div>

      <div class="row">
      	<div class="col-6">
      	  <p>
      	    Задача извлечения именованных сущностей (NER) старая и хорошо изученная.
      	  </p>

      	  <p>
      	    Для английского языка с 1997 года проводятся соревнования: <a href="https://www-nlpir.nist.gov/related_projects/muc/proceedings/ne_task.html">MUC-7</a>, <a href="https://www.aclweb.org/anthology/W03-0419/">CoNLL-2003</a>, <a href="https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf">OntoNotes 5</a>.

      	     Существует масса коммерческих и открытых решений: <a href="https://spacy.io/">Spacy</a>, <a href="https://nlp.stanford.edu/software/CRF-NER.shtml">Stanford NER</a>, <a href="https://opennlp.apache.org/">OpenNLP</a>, <a href="http://www.nltk.org/">NLTK</a>, <a href="https://github.com/mit-nlp/MITIE">MITIE</a>, <a href="https://cloud.google.com/natural-language/">Google Natural Language API</a>, <a href="https://www.paralleldots.com/named-entity-recognition">ParallelDots</a>, <a href="https://aylien.com/text-api/">Aylien</a>, <a href="https://www.basistech.com/text-analytics/rosette/entity-extractor/">Rosette</a>, <a href="https://www.textrazor.com/">TextRazor</a>.
      	  </p>

      	  <p>
      	    Для русского тоже есть бенчмарки: <a href="http://www.labinform.ru/pub/named_entities/">Collection5</a>, <a href="https://www.researchgate.net/publication/262203599_Introducing_Baselines_for_Russian_Named_Entity_Recognition">Gareev</a>, <a href="https://github.com/dialogue-evaluation/factRuEval-2016/">factRuEval-2016</a>, <a href="https://www.aclweb.org/anthology/I17-1042/">WiNER</a>, <a href="http://bsnlp.cs.helsinki.fi/shared_task.html">BSNLP-2019</a>.

      	    Решения в основном закрытые: <a href="https://dadata.ru/">DaData</a>, <a href="http://pullenti.ru/?AspxAutoDetectCookieSupport=1">Pullenti</a>, <a href="https://www.abbyy.com/ru-ru/infoextractor/">Abbyy Infoextractor</a>, <a href="http://dictum.ru/ru/named-entities-extraction/blog">Dictum</a>, <a href="http://eurekaengine.ru/">Eureka</a>, <a href="http://www.promt.ru/press/news/57586/">Promt</a>, <a href="http://www.rco.ru/?page_id=3554">RCO</a>, <a href="http://www.ixlab.ru/site/onwork">Ahunter</a>.
      	  </p>

      	  <p>
      	    В конце 2018 года после <a href="https://arxiv.org/abs/1810.04805">статьи от Google про BERT</a> в англоязычном NLP случился большой прогресс. В 2019 ребята из <a href="https://deeppavlov.ai/">проекта DeepPavlov</a> адаптировали мультиязычный BERT для русского, появился <a href="http://docs.deeppavlov.ai/en/master/features/models/bert.html">RuBERT</a>. Поверх обучили <a href="http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf">CRF-голову</a>, получился <a href="https://www.youtube.com/watch?v=eKTA8i8s-zs">DeepPavlov BERT NER</a> — SOTA для русского языка. У модели великолепное качество, в 2 раза меньше ошибок, чем у ближайшего преследователя <a href="https://github.com/deepmipt/ner">DeepPavlov NER</a>, но размер и производительность пугают: 6ГБ — потребление GPU RAM, 2ГБ — размер модели, 13 статей в секунду — производительность на хорошей GPU.
      	  </p>

      	  <p>
      	    В 2020 году в проекте Natasha нам удалось вплотную приблизится по качеству к DeepPavlov BERT NER, размер модели получился в 75 раз меньше (27МБ), потребление памяти в 30 раз меньше (205МБ), скорость в 2 раза больше на CPU (25 статей в секунду).
      	  </p>
      	</div>

      	<div class="col-6">
      	  <table class="table table-borderless">
      	    <thead>
      	      <tr>
      		<th></th>
      		<th scope="col">Natasha (Slovnet NER)</th>
      		<th scope="col">DeepPavlov BERT NER</th>
      	      </tr>
      	    </thead>
      	    <tbody>
      	      <tr>
      		<td>PER/LOC/ORG F1 по токенам, среднее по Collection5, factRuEval-2016, BSNLP-2019, Gareev</td>
      		<td>0.97/0.91/0.85</td>
      		<td>0.98/0.92/0.86</td>
      	      </tr>

      	      <tr>
      		<td>Размер модели</td>
      		<td>27МБ</td>
      		<td>2ГБ</td>
      	      </tr>

      	      <tr>
      		<td>Потребление памяти</td>
      		<td>205МБ</td>
      		<td>6ГБ (GPU)</td>
      	      </tr>

      	      <tr>
      		<td>Производительность, новостных статей в секунду (1 статья ≈ 1КБ)</td>
      		<td>25 на CPU (Core i5)</td>
      		<td>13 на GPU (RTX 2080 Ti), 1 на CPU</td>
      	      </tr>

      	      <tr>
      		<td>Время инициализации, секунд</td>
      		<td>1</td>
      		<td>35</td>
      	      </tr>

      	      <tr>
      		<td></td>
      		<td>Python 3.5+, PyPy3</td>
      		<td>Python 3.6+</td>
      	      </tr>

      	      <tr>
      		<td></td>
      		<td>NumPy</td>
      		<td>TensorFlow</td>
      	      </tr>
      	    </tbody>
      	  </table>
      	  <div class="note">
      	    Natasha использует NER-модель из библиотеки <a href="https://github.com/natasha/slovnet">Slovnet</a>. Качество на 1 процентный пункт ниже, чем у SOTA DeepPavlov BERT NER, размер модели в 75 раз меньше, потребление памяти в 30 раз меньше, скорость в 2 раза больше на CPU. Сравнение со SpaCy, PullEnti и другими решениями для русскоязычного NER в <a href="https://github.com/natasha/slovnet#ner-1">репозитории Slovnet</a>.
      	  </div>
      	</div>
      </div>

      <div class="row">
      	<div class="col-6">
      	  <p>
      	    Как получить такой результат? Короткий рецепт:
      	  </p>

      	  <p class="highlight">
      	    Natasha (<a href="https://github.com/natasha/slovnet#ner">Slovnet NER</a>) = <a href="https://github.com/natasha/slovnet/blob/master/scripts/02_bert_ner/main.ipynb">Slovnet BERT NER</a> — аналог DeepPavlov BERT NER + дистилляция через синтетическую разметку (<a href="https://github.com/natasha/nerus">Nerus</a>) в WordCNN-CRF c квантованными эмбеддингами (<a href="https://github.com/natasha/navec">Navec</a>) + движок для инференса на NumPy.
      	  </p>
      	</div>
      </div>

      <div class="row">
      	<div class="col-6">
      	  <p>
      	    Теперь по порядку. План такой: обучим тяжёлую модель c BERT-архитектурой на небольшом вручную аннотированном датасете. Разметим ей корпус новостей, получится большой грязный синтетический тренировочный датасет. Обучим на нём компактную примитивную модель. Этот процесс называется дистилляцией: тяжёлая модель — учитель, компактная — ученик. Рассчитываем, что BERT-архитектура избыточна для задачи NER, компактная модель несильно проиграет по качеству тяжёлой.
      	  </p>

      	  <p>
      	    В проекте Natasha библиотека <a href="https://github.com/natasha/slovnet">Slovnet</a> занимается обучением и инференсом современных моделей для русскоязычного NLP. Используем Slovnet для подготовки моделей учителя и ученика.
      	  </p>
      	</div>
      </div>

      <h2>Модель-учитель</h2>
      
      <div class="row">
      	<div class="col-6">
      	  <p>
      	    DeepPavlov BERT NER состоит из RuBERT-энкодера и CRF-головы. Наша тяжёлая модель-учитель повторяет эту архитектуру с небольшими улучшениями.
	  </p>
	  <p>
	    Все бенчмарки измеряют качество NER на текстах новостей. Дообучим RuBERT на новостях. В репозитории <a href="https://github.com/natasha/corus">Corus</a> собраны ссылки на публичные русскоязычные новостные корпуса, в сумме 12 ГБ текстов. Используем техники из <a href="https://arxiv.org/abs/1907.11692">статьи от Facebook про RoBERTa</a>: большие агрегированные батчи, динамическая маска, отказ от предсказания следующего предложения (NSP). RuBERT использует огромный словарь на 120&nbsp;000 сабтокенов — наследие мультиязычного BERT от Google. Сократим размер до 50&nbsp;000 самых частотных для новостей, покрытие уменьшится на 5%. Получим <a href="https://github.com/natasha/slovnet/blob/master/scripts/01_bert_news/main.ipynb">NewsRuBERT</a>, модель предсказывает замаскированные сабтокены в новостях на 5 процентных пунктов лучше RuBERT (63% в топ-1).
      	  </p>

      	  <p>
      	    Обучим NewsRuBERT-энкодер и CRF-голову на 1000 статей из <a href="https://github.com/natasha/corus#load_ne5">Collection5</a>. Получим <a href="https://github.com/natasha/slovnet/blob/master/scripts/02_bert_ner/main.ipynb">Slovnet BERT NER</a>, качество на 0.5 процентных пункта лучше, чем у DeepPavlov BERT NER, размер модели меньше в 4 раза (473МБ), работает в 3 раза быстрее (40 статей в секунду).
      	  </p>

      	  <p class="highlight">
      	    NewsRuBERT = RuBERT + 12ГБ новостей + техники из RoBERTa + 50K-словарь.
      	    <br/>
      	    Slovnet BERT NER (аналог DeepPavlov BERT NER) = NewsRuBERT + CRF-голова + Collection5.
      	  </p>

      	</div>

      	<div class="note col-6">
      	  <ul>
      	    <li>
      	      С PyTorch 1.4 <a href="https://github.com/natasha/slovnet/blob/c4f961c23ba0eebb216cae3dc79817e8e6600a6f/slovnet/model/bert.py#L46-L106">реализация NewsRuBERT</a> занимает 100 строк на Python;
      	    </li>
      	    <li>
      	      <a href="https://github.com/natasha/slovnet#ner-1">Сравнение качества и производительности</a> DeepPavlov BERT NER, Slovnet BERT NER и других решений задачи NER для русского языка.
      	    </li>
      	  </ul>
      	</div>
      </div>

      <h2>Синтетический датасет</h2>

      <div class="row">
      	<div class="col-6">
      	  <p>
      	    Разметим 700&nbsp;000 статей из <a href="https://github.com/natasha/corus#load_lenta">корпуса Lenta.ru</a> тяжёлой моделью. Получим огромный синтетический обучающий датасет. Архив доступен в репозитории <a href="https://github.com/natasha/nerus">Nerus</a> проекта Natasha. Разметка очень качественная, оценки F1 по токенам: PER — 99.7%, LOC — 98.6%, ORG — 97.2%. Редкие примеры ошибок:
      	  </p>
      	</div>
      </div>

      <div id="errors" class="row">
      	<div class="col-6">
      	  <p>
      	    Выборы <span class="box ORG">Верховного совета</span> <span class="box LOC">Аджарской автономной республики</span> назначены в соответствии с 241-ой статьей и 4-м пунктом 10-й статьи Конституционного закона <span class="box LOC">Грузии</span> О статусе <span class="box LOC">Аджарской автономной</span><span class="error"> республики</span>.
      	  </p>
      	  <p>
      	    <span class="box ORG">Следственное управление</span><span class="error"> при прокуратуре</span> требует наказать премьера <span class="box LOC">Якутии</span>.
      	  </p>
      	  <p>
      	    Страны <span class="error">Азии</span> и <span class="error">Африки</span> поддержали позицию <span class="box LOC">России</span> в конфликте с <span class="box LOC">Грузией</span>.
      	  </p>
      	  <p>
      	    Как сообщили в четверг корреспонденту <span class="box ORG">Агентства национальных новостей</span> в следственном управлении, еще 16 мая 2007 г. прокуратурой <span class="box LOC">Якутии</span> было возбуждено уголовное дело № 66144 по признакам преступления, предусмотренного ч. 4 ст. 159 УК <span class="box LOC">РФ</span> по факту причинения имущественного ущерба в размере 30 млн руб. государственному унитарному предприятию "<span class="box ORG">Дирекция по строительству</span><span class="error"> железной дороги</span> "<span class="box LOC">Беркакит-Томмот-Якутск</span>"".
      	  </p>
      	</div>
      	<div class="col-6">
      	  <p>
      	    У <span class="box PER">Владимира Стржалковского</span> появится помощник - специалист по проведению сделок <span class="error"><span class="box ORG">M</span>&A</span>.
      	  </p>
      	  <p>
      	    Начальник полигона твердых бытовых отходов "<span class="error">Игумново</span>" в <span class="box LOC">Нижегородской области</span> осужден за загрязнение атмосферы и грунтовых вод.
      	  </p>
      	  <p>
      	    Постоянный <span class="error">Секретариат</span> <span class="box ORG">ОССНАА</span> в <span class="box LOC">Каире</span> в пятницу заявил: Когда <span class="box PER">Саакашвили</span> стал президентом <span class="box LOC">Грузии</span>, он проявил стремление вступить в <span class="box ORG">НАТО</span>, <span class="box LOC">Европейский Союз</span> и установить более близкие отношения с <span class="box LOC">США</span>.
      	  </p>

      	  <div class="note">
      	    Примеры ошибок большого грязного обучающего датасета <a href="https://github.com/natasha/nerus">Nerus</a>. Сходу не всегда понятно в чём проблема, пропущенные участки помечены красным пунктиром.
      	  </div>
      	</div>
      </div>

      <h2>Модель-ученик</h2>

      <div class="row">
	<div class="col-6">
	  <p>
	    С выбором архитектуры тяжёлой модели-учителя проблем не возникло, вариант один — трансформеры. С компактной моделью-учеником сложнее, вариантов много. С 2013 до 2018 год с появления word2vec до статьи про BERT, человечество придумало кучу нейросетевых архитектур для решения задачи NER. У всех общая схема:
	  </p>

	  <!--
https://latex.codecogs.com/eqneditor/editor.php
https://viereck.ch/latex-to-svg/

\begin{pmatrix}
    \text{CharCNN} \\
    \text{CharRNN} \\
    \text{Embedding} \\
\end{pmatrix}
-
\begin{pmatrix}
    \text{WordCNN} \\
    \text{WordRNN} \\
\end{pmatrix}
-
\begin{pmatrix}
    \text{Linear} \\
    \text{CRF} \\
\end{pmatrix}
p	    -->
	  <p>
	    <img src="images/comb.svg" class="img-fluid"/>
	    <div class="note">Схема нейросетевых архитектур для задачи NER: энкодер токенов, энкодер контекста, декодер тегов. Расшифровка сокращений в <a href="https://arxiv.org/pdf/1806.05626.pdf">обзорной статье Yang (2018)</a>.</div>
	  </p>

	  <p>
	    Комбинаций архитектур много. Какую выбрать? Например, (CharCNN + Embedding)-WordBiLSTM-CRF — схема модели из <a href="https://arxiv.org/pdf/1709.09686.pdf">статьи про DeepPavlov NER</a>, SOTA для русского языка до 2019 года.
	  </p>

	  <p>
	    Варианты с CharCNN, CharRNN пропускаем, запускать маленькую нейронную сеть по символам на каждом токене — не наш путь, слишком медленно. WordRNN тоже хотелось бы избежать, решение должно работать на CPU, перемножать матрицы на каждом токене медленно. Для NER выбор между Linear и CRF условный. Мы используем BIO-кодировку, порядок тегов важен. Приходится терпеть жуткие тормоза, использовать CRF. Остаётся один вариант — Embedding-WordCNN-CRF. Такая модель не учитывает регистр токенов, для NER это важно, "надежда" — просто слово, "Надежда" — возможно, имя. Добавим ShapeEmbedding — эмбеддинг с очертаниями токенов, например: "NER" — EN_XX, "Вайнович" — RU_Xx, "!" — PUNCT_!, "и" — RU_x, "5.1" — NUM, "Нью-Йорк" — RU_Xx-Xx. Схема Slovnet NER — (WordEmbedding + ShapeEmbedding)-WordCNN-CRF.
	  </p>
	</div>

	<div class="note col-6">
	  <p>
	    Что такое CharCNN, CharRNN, WordCNN, WordRNN, CRF? Основные статьи на тему:
	  </p>

	  <ul>
	    <li>
	      2009 Ratinov, <a href="https://www.aclweb.org/anthology/W09-1119.pdf">Design Challenges and Misconceptions in Named Entity Recognition</a> — обзор донейросетевых методов;
	    </li>

	    <li>
	      2011 Collobert, <a href="https://arxiv.org/pdf/1103.0398.pdf">Natural Language Processing (almost) from Scratch</a> — первое применение сетей, знаковая работа, автор всё придумал уже 10 лет назад;
	    </li>

	    <li>
	      2015 Huang, <a href="https://arxiv.org/pdf/1508.01991.pdf">Bidirectional LSTM-CRF Models for Sequence Tagging</a> — первое примение WordBiLSTM-CRF;
	    </li>

	    <li>
	      2016 Ling, <a href="https://arxiv.org/pdf/1508.02096.pdf">Compositional Character Models for Open Vocabulary Word Representation</a> — CharBiLSTM;
	    </li>

	    <li>
	      2016 Chiu, Nichols, <a href="https://www.aclweb.org/anthology/Q16-1026.pdf">Named Entity Recognition with Bidirectional LSTM-CNNs</a> — CharCNN;
	    </li>

	    <li>
	      2016 Ma, Hovy, <a href="https://arxiv.org/pdf/1603.01354.pdf">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</a> — CharCNN-WordBiLSTM-CRF;
	    </li>

	    <li>
	      2016 Lampe, <a href=" https://arxiv.org/pdf/1603.01360.pdf">Neural Architectures for Named Entity Recognition</a> — WordStackLSTM, экзотический вариант, используется в SpaCy;
	    </li>

	    <li>
	      2017 Yang, <a href=" https://arxiv.org/pdf/1703.06345.pdf">Transfer learning for sequence tagging with hierarchical recurrent networks</a> — первые попытки применить transfer learing;
	    </li>

	    <li>
	      2018 Peters, <a href="https://arxiv.org/pdf/1802.05365.pdf">Deep contextualized word representations</a> — transfer learing через language modeling;
	    </li>

	    <li>
	      2019, <a href="https://arxiv.org/pdf/1909.00100.pdf">Small and Practical BERT Models for Sequence Labeling</a> — BERT;
	    </li>
	  </ul>
	</div>
      </div>

      <h2>Дистилляция</h2>

      <div class="row">
	<div class="col-6">
	  <p>
	    Обучим Slovnet NER на огромном синтетическом датасете. Сравним результат с тяжёлой моделью-учителем Slovnet BERT NER. Качество считаем и усредняем по размеченным вручную Collection5, Gareev, factRuEval-2016, BSNLP-2019. Размер обучающей выборки очень важен: на 250 новостных статьях (размер factRuEval-2016) средний по PER, LOC, LOG F1 — 0.64, на 1000 (аналог Collection5) — 0.81, на всём датасете — 0.91, качество Slovnet BERT NER — 0.92. Примитивная модель-ученик на 1 процентный пункт хуже тяжёлой модели-учителя. Это замечательный результат. Напрашивается универсальный рецепт:
	  </p>

	  <p class="highlight">
	    Вручную размечаем немного данных. Обучаем тяжёлый трансформер. Генерируем много синтетических данных. Обучаем простую модель на большой выборке. Получаем качество трансформера, размер и производительность простой модели.
	  </p>

	  <p>
	    В библиотеке Slovnet есть ещё две модели обученные по этому рецепту: <a href="https://github.com/natasha/slovnet#morphology">Slovnet Morph</a> — морфологический теггер, <a href="https://github.com/natasha/slovnet#syntax">Slovnet Syntax</a> — синтаксический парсер. Slovnet Morph отстаёт от тяжёлой модели-учителя <a href="https://github.com/natasha/slovnet#morphology-1">на 2 процентных пункта</a>, Slovnet Syntax — <a href="https://github.com/natasha/slovnet#syntax-1">на 5</a>. У обеих моделей качество и производительность выше существующих решений для русского на новостных статьях.
	  </p>
	</div>
	<div class="col-6">
	  <p>
	    <img src="images/size.svg" class="img-fluid"/>
	  </p>
	  <div class="note">
	    Качество Slovnet NER, зависимость от числа синтетических обучающих примеров. Серая линия — качество Slovnet BERT NER. Slovnet NER не видит размеченных вручную примеров, обучается только на синтетических данных.
	  </div>
	</div>
      </div>

      <h2>Квантизация</h2>

      <div class="row">
	<div class="col-6">
	  <p>
	    Размер Slovnet NER — 289МБ. 287МБ занимает таблица с эмбеддингами. Модель использует большой словарь на 250&nbsp;000 строк, он покрывает 98% слов в новостных текстах. Используем <a href="http://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/">квантизацию</a>, заменим 300-мерные float-вектора на 100-мерные 8-битные. Размер модели уменьшится в 10 раз (27МБ), качество не изменится. <a href="https://github.com/natasha/navec">Библиотека Navec</a> — часть проекта Natasha, коллекция квантованных предобученных эмбеддингов. <a href="https://github.com/natasha/navec#hudlit">Веса обученные на художественной литературе</a> занимают 50МБ, обходят по <a href="https://github.com/natasha/navec/#evaluation">синтетическим оценкам</a> все <a href="https://rusvectores.org/ru/models/">статические модели RusVectores</a>.
	  </p>
	</div>

	<div class="col-6">
	  <!-- {\huge 289} \xrightarrow{\text{quantization}}{\huge\text{27MB}} -->
	  <img src="images/quant.svg" class="img-fluid"/>
	</div>

      </div>

      <h2>Инференс</h2>

      <div class="row">
	<div class="col-6">
	  <p>
	    Slovnet NER использует PyTorch для обучения. Пакет PyTorch весит 700МБ, не хочется тянуть его в продакшн для инференса. Ещё PyTorch <a href="https://github.com/pytorch/pytorch/issues/17835">не работает с интерпретатором PyPy</a>. Slovnet используется в связке с <a href="https://github.com/natasha/yargy">Yargy-парсером</a> аналогом <a href="https://yandex.ru/dev/tomita/">яндексового Tomita-парсера</a>. С PyPy Yargy работает в 2-10 раз быстрее, зависит от сложности грамматик. Не хочется терять скорость из-за зависимости от PyTorch.
	  </p>
	  <p>
	    Стандартное решение — использовать <a href="https://pytorch.org/docs/stable/jit.html">TorchScript</a> или <a href="https://pytorch.org/docs/stable/onnx.html">сконвертировать модель в ONNX</a>, инференс делать в <a href="https://github.com/microsoft/onnxruntime">ONNXRuntime</a>. Slovnet NER использует нестандартные блоки: квантованные эмбеддинги, CRF-декодер. TorchScript и ONNXRuntime не поддерживают PyPy.
	  </p>
	  <p>
	    Slovnet NER — простая модель, <a href="https://github.com/natasha/slovnet/blob/master/slovnet/exec/model.py">вручную реализуем все блоки на NumPy</a>, используем веса, посчитанные PyTorch. Применим немного NumPy-магии, аккуратно реализуем <a href="https://github.com/natasha/slovnet/blob/master/slovnet/exec/model.py#L82-L112">блок CNN</a>, <a href="https://github.com/natasha/slovnet/blob/master/slovnet/exec/model.py#L154-L184">CRF-декодер</a>, распаковка квантованного эмбеддинга <a href="https://github.com/natasha/slovnet/blob/master/slovnet/exec/model.py#L229-L234">занимает 5 строк</a>. Скорость инференса на CPU такая же как с ONNXRuntime и PyTorch, 25 новостных статей в секунду на Core i5.
	  </p>
	  <p>
	    Техника работает на более сложных моделях: Slovnet Morph и Slovnet Syntax тоже реализованы на NumPy. Slovnet NER, Morph и Syntax используют общую таблицу эмбеддингов. Вынесем веса в отдельный файл, таблица не дублируется в памяти и на диске:
	    <pre>
>>> navec = Navec.load('navec_news_v1_1B.tar')  # 25MB
>>> morph = Morph.load('slovnet_morph_news_v1.tar')  # 2MB
>>> syntax = Syntax.load('slovnet_syntax_news_v1.tar')  # 3MB
>>> ner = NER.load('slovnet_ner_news_v1.tar')  # 2MB

# 25 + 2 + 3 + 2 вместо 25+2 + 25+3 + 25+2
>>> morph.navec(navec)
>>> syntax.navec(navec)
>>> ner.navec(navec)</pre>
	  </p>
	</div>
      </div>

      <h2>Использование</h2>

      <div class="row">
	<div class="col-6">
	  <p>
	    <a href="https://github.com/natasha/natasha#install">Инструкция по установке</a>, <a href="https://github.com/natasha/natasha#usage">пример использования</a>, <a href="http://nbviewer.jupyter.org/github/natasha/natasha/blob/master/docs.ipynb">документация</a> — в <a href="https://github.com/natasha/natasha">репозитории библиотеки Natasha</a>.
	  </p>

	  <p>
	    Natasha объединяет под одним интерфейсом другие библиотеки проекта. Natasha хороша для демонстрации технологий. В работе стоит использовать низкоуровневые библиотеки напрямую:
	    <ul>
	      <li>
		<a href="https://github.com/natasha/razdel">Razdel</a> — сегментация текста на предложения и токены;
	      </li>
	      <li>
		<a href="https://github.com/natasha/navec">Navec</a> — качественный компактные эмбеддинги;
	      </li>
	      <li>
		<a href="https://github.com/natasha/slovnet">Slovnet</a> — современные компактные модели для морфологии, синтаксиса, NER;
	      </li>
	      <li>
		<a href="https://github.com/natasha/yargy">Yargy</a> — правила и словари для извлечения структурированной информации;
	      </li>
	      <li>
		<a href="https://github.com/natasha/ipymarkup">Ipymarkup</a> — визуализация NER и синтаксической разметки;
	      </li>
	      <li>
		<a href="https://github.com/natasha/corus">Corus</a> — коллекция ссылок на публичные русскоязычные датасеты;
	      </li>
	      <li>
		<a href="https://github.com/natasha/nerus">Nerus</a> — большой корпус с автоматической разметкой именованных сущностей, морфологии и синтаксиса.
	      </li>
	    </ul>
	  </p>

	  <p>
	    Natasha извлекает стандартные сущности: имена, названия топонимов и организаций. Решение показывает хорошее качество на новостях. Как работать с другими сущностями и типами текстов? Нужно обучить новую модель. Сделать это непросто. За компактный размер и скорость работы мы платим сложностью подготовки модели. <a href="https://github.com/natasha/slovnet/blob/master/scripts/02_bert_ner/main.ipynb">Скрипт-ноутбук для подготовки тяжёлой модели учителя</a>, <a href="https://github.com/natasha/slovnet/blob/master/scripts/05_ner/main.ipynb">скрипт-ноутбук для модели-ученика</a>, <a href="https://github.com/natasha/navec#development">инструкции по подготовке квантованных эмбеддингов</a>.
	  </p>

	  <p>
	    <a href="https://t.me/natural_language_processing"><img class="inline" src="/images/social/tg.svg" /> natural_language_processing</a> — чат пользователей, разработчиков проекта.
	  </p>
	</div>
      </div>

      <div id="footer">
	<div class="row">
      	  <div class="col-2">
      	    <img src="/images/author.jpg" class="rounded img-fluid" alt="Александр Кукушкин">
      	  </div>
      	  <div class="col-4">
      	    Александр Кукушкин, июнь 2020 года
      	    <ul>
      	      <li>
		<a href="mailto:alex@alexkuk.ru">alex@alexkuk.ru</a>
	      </li>

      	      <li>
		<a href="https://t.me/alexkuk">
		  <img class="inline" src="/images/social/tg.svg" /> alexkuk
		</a>
	      </li>

      	      <li>
		<a href="https://github.com/kuk">
		  <img class="inline" src="/images/social/gh.svg" /> kuk
		</a>
	      </li>
      	    </ul>

	    <a href="https://lab.alexkuk.ru/">Лаборатория анализа данных Александра Кукушкина</a>
	      <p>
		Лаборатория разрабатывает сервисы и коробочные продукты с использованием технологии Natasha, оказывает услуги анализа данных для российских компаний.
	      </p>

	  </div>
	</div>
      </div>

    </div>

    <script src="/scripts/jquery-3.5.1.min.js" type="text/javascript"></script>
    <script src="/scripts/popper.min.js" type="text/javascript"></script>
    <script src="/scripts/bootstrap.min.js" type="text/javascript"></script>
    <script src="script.js" type="text/javascript"></script>
  </body>
</html>
